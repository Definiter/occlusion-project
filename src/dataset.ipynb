{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate training dataset and test dataset.\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import copy\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "new_to_original_class_id = [99, 113, 132, 144, 321, 344, 386, 494, 497, 512, 525, 526, \\\n",
    "650, 656, 737, 763, 844, 848, 903, 945]\n",
    "\n",
    "original_to_new_class_id = {512: 9, 321: 4, 386: 6, 99: 0, 132: 2, 848: 17, 497: 8,\\\n",
    "650: 12, 903: 18, 844: 16, 525: 10, 494: 7, 144: 3, 113: 1, 763: 15, 526: 11, 344: 5,\\\n",
    "656: 13, 737: 14, 945: 19}\n",
    "\n",
    "imagenet_root = '/home/haow3/occlusion-project/data/imagenet/'\n",
    "caffe_root = '/home/haow3/software/caffe-rc3/'\n",
    "\n",
    "# Load labels.\n",
    "imagenet_labels_filename = caffe_root + 'data/ilsvrc12/synset_words.txt'\n",
    "label_to_wnid = np.loadtxt(imagenet_labels_filename, str, delimiter='\\t')\n",
    "wnid_to_label = {}\n",
    "for label in range(len(label_to_wnid)):\n",
    "    wnid = label_to_wnid[label].split(' ')[0]\n",
    "    wnid_to_label[wnid] = label\n",
    "\n",
    "\n",
    "training_dataset = [(0.0, 0), (1.0/3, 3), (1.0/2, 3), (2.0/3, 2), (1.0, 1)]\n",
    "test_dataset = [(0.0, 0), (1.0/3, 3), (1.0/2, 3), (2.0/3, 2), (1.0, 1)]\n",
    "#training_dataset = []\n",
    "#test_dataset = [(1.0/2, 3)]\n",
    "\n",
    "train_folders = []\n",
    "test_folders = []\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "for (occlu_size, occlu_num) in training_dataset:\n",
    "    percent = str(int(100 * occlu_size * occlu_size))\n",
    "    print percent\n",
    "    f = open('{}dataset/train_{}_{}.txt'.format(imagenet_root, percent, str(occlu_num)), 'w')\n",
    "    train_files.append(f)\n",
    "    \n",
    "    folder = '{}dataset/train_{}_{}/'.format(imagenet_root, percent, str(occlu_num))\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    train_folders.append(folder)\n",
    "        \n",
    "\n",
    "for (occlu_size, occlu_num) in test_dataset:\n",
    "    percent = str(int(100 * occlu_size * occlu_size))\n",
    "    print percent\n",
    "    f = open('{}dataset/test_{}_{}.txt'.format(imagenet_root, percent, str(occlu_num)), 'w')\n",
    "    test_files.append(f)\n",
    "\n",
    "    folder = '{}dataset/test_{}_{}/'.format(imagenet_root, percent, str(occlu_num))\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    test_folders.append(folder)\n",
    "    \n",
    "# occluder size = occlu_size * occlu_size\n",
    "# occluder num = occlu_num * occlu_num\n",
    "# path = 'imagenet_root/dataset/train_0/name'\n",
    "def generate_datum(img_orig, path, f, class_id, rects, occlu_size, occlu_num):\n",
    "    if occlu_size == 0:\n",
    "        datum_path = '{}_0.jpeg'.format(path)\n",
    "        img_orig.save(datum_path)\n",
    "        f.write('{} {}\\n'.format(datum_path, str(class_id)))\n",
    "        return\n",
    "    for i in range(occlu_num):\n",
    "        for j in range(occlu_num):\n",
    "            img = copy.copy(img_orig)\n",
    "            d = ImageDraw.Draw(img)\n",
    "            if (occlu_num == 1):\n",
    "                delta = 1\n",
    "            else:\n",
    "                delta = (1 - occlu_size) / float(occlu_num - 1)\n",
    "            for rect in rects:\n",
    "                subrect = [0, 0, 0, 0]\n",
    "                subrect[0] = rect[0] + i * (rect[2] - rect[0]) * delta\n",
    "                subrect[1] = rect[1] + j * (rect[3] - rect[1]) * delta\n",
    "                subrect[2] = subrect[0] + (rect[2] - rect[0]) * occlu_size\n",
    "                subrect[3] = subrect[1] + (rect[3] - rect[1]) * occlu_size\n",
    "                d.rectangle(subrect, fill=\"black\", outline=None)\n",
    "            datum_path = '{}_{}_{}_{}_{}.jpeg'.format(path, str(int(100 * occlu_size * occlu_size)), str(occlu_num), str(i), str(j))\n",
    "            img.save(datum_path)\n",
    "            f.write('{} {}\\n'.format(datum_path, str(class_id)))\n",
    "            \n",
    "synset_names = os.listdir(imagenet_root + 'image/')\n",
    "             \n",
    "for synset_name in synset_names:\n",
    "    image_names = os.listdir(imagenet_root + 'image/' + synset_name + '/' + synset_name + '_original_images')\n",
    "    annotation_names = os.listdir(imagenet_root + 'Annotation/' + synset_name + '/')\n",
    "    n1 = [os.path.splitext(n)[0] for n in image_names]\n",
    "    n2 = [os.path.splitext(n)[0] for n in annotation_names]\n",
    "    intersection_names = list(set(n1) & set(n2))\n",
    "    # train : test = 300 : 100\n",
    "    for i in range(400):\n",
    "        print 'Processing: ', intersection_names[i]\n",
    "        # Read bounding box.\n",
    "        bbx_file = open(imagenet_root + 'Annotation/' + synset_name + '/' + intersection_names[i] + '.xml')\n",
    "        xmltree = ET.parse(bbx_file)\n",
    "        objects = xmltree.findall('object')\n",
    "        rects = []\n",
    "        for obj in objects:\n",
    "            bbx = obj.find('bndbox')\n",
    "            rects.append([int(it.text) for it in bbx])\n",
    "            \n",
    "        img_orig = Image.open(imagenet_root + 'image/' + synset_name + '/' + synset_name + '_original_images/' + intersection_names[i] + '.JPEG')\n",
    "        if i < 300: # Training dataset. \n",
    "            for index, (occlu_size, occlu_num) in enumerate(training_dataset):\n",
    "                generate_datum(img_orig, '{}{}'.format(train_folders[index], intersection_names[i]), \\\n",
    "                               train_files[index], original_to_new_class_id[wnid_to_label[synset_name]], rects, occlu_size, occlu_num)\n",
    "        else: # Testing dataset.\n",
    "            for index, (occlu_size, occlu_num) in enumerate(test_dataset):\n",
    "                generate_datum(img_orig, '{}{}'.format(test_folders[index], intersection_names[i]), \\\n",
    "                               test_files[index], original_to_new_class_id[wnid_to_label[synset_name]], rects, occlu_size, occlu_num)\n",
    "            \n",
    "for f in train_files:\n",
    "    f.close()\n",
    "for f in test_files:\n",
    "    f.close()\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data cleaning.\n",
    "'''\n",
    "Some constants:\n",
    "\n",
    "synset_names = ['n01855672', 'n02009912', 'n02051845', 'n02398521', 'n03017168',\\\n",
    "'n03028079', 'n03109150', 'n03160309', 'n03179701', 'n03759954', 'n03770679', \\\n",
    "'n03983396', 'n04086273', 'n04372370', 'n04392985', 'n04584207', 'n07720875', \\\n",
    "'n02504458', 'n02276258', 'n01944390']\n",
    "\n",
    "new_to_original_class_id = [99, 113, 132, 144, 321, 344, 386, 494, 497, 512, 525, 526, \\\n",
    "650, 656, 737, 763, 844, 848, 903, 945]\n",
    "\n",
    "original_to_new_class_id = {512: 9, 321: 4, 386: 6, 99: 0, 132: 2, 848: 17, 497: 8,\\\n",
    "650: 12, 903: 18, 844: 16, 525: 10, 494: 7, 144: 3, 113: 1, 763: 15, 526: 11, 344: 5,\\\n",
    "656: 13, 737: 14, 945: 19}\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "imagenet_root = '/home/haow3/occlusion-project/data/imagenet/'\n",
    "caffe_root = '/home/haow3/software/caffe-rc3/'\n",
    "\n",
    "original_to_new_class_id = {512: 9, 321: 4, 386: 6, 99: 0, 132: 2, 848: 17, 497: 8,\\\n",
    "650: 12, 903: 18, 844: 16, 525: 10, 494: 7, 144: 3, 113: 1, 763: 15, 526: 11, 344: 5,\\\n",
    "656: 13, 737: 14, 945: 19}\n",
    "\n",
    "for filename in os.listdir(imagenet_root + 'dataset/list_original_id/'):\n",
    "    if filename.find('.txt') == -1:\n",
    "        continue\n",
    "    print 'Processing: ', filename\n",
    "    \n",
    "    fin = open(imagenet_root + 'dataset/list_original_id/' + filename, 'r')\n",
    "    fout = open(imagenet_root + 'dataset/' + filename, 'w')\n",
    "    \n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        filepath, class_id = line.split(' ')\n",
    "        fout.write('{} {}\\n'.format(filepath, original_to_new_class_id[int(class_id)]))\n",
    "        \n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
